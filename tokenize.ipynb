{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bde5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd6ee42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In colab? False\n"
     ]
    }
   ],
   "source": [
    "def is_running_on_colab():\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Set a global flag\n",
    "IN_COLAB = is_running_on_colab()\n",
    "print('In colab?',IN_COLAB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d64ccf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_path_train_df= \"/content/drive/MyDrive/bail_prediction_datasets/train_all_ranked.csv\" if IN_COLAB else 'csv_datasets/train_all_ranked.csv'\n",
    "load_path_val_df= \"/content/drive/MyDrive/bail_prediction_datasets/val_all_ranked.csv\" if IN_COLAB else 'csv_datasets/val_all_ranked.csv'\n",
    "train_df = pd.read_csv(load_path_train_df)\n",
    "val_df = pd.read_csv(load_path_val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2de00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"id\", \"text\",\"district\"], inplace=True)\n",
    "val_df.drop(columns=[\"id\", \"text\",\"district\"], inplace=True)\n",
    "train_df.rename(columns={\"ranked-sentences\": \"text\"}, inplace=True)\n",
    "val_df.rename(columns={\"ranked-sentences\": \"text\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbab7088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123742/123742 [00:03<00:00, 32473.14it/s]\n",
      "100%|██████████| 17707/17707 [00:00<00:00, 30266.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# This runs once on CPU so the GPU doesn't have to wait for 'ast.literal_eval' later\n",
    "def pre_process_df(df):\n",
    "    # Use progress_apply so you can see the bar\n",
    "    df[\"text\"] = df[\"text\"].progress_apply(lambda x: \" \".join(ast.literal_eval(x)[:10]))\n",
    "    return df\n",
    "\n",
    "print(\"Pre-processing text...\")\n",
    "train_df = pre_process_df(train_df)\n",
    "val_df = pre_process_df(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4887366",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_train_df = train_df.sample(frac = 0.1, random_state=42).reset_index()\n",
    "hp_val_df = val_df.sample(frac = 0.1, random_state=42).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72f3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LegalDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer):\n",
    "#         self.labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "#         print(\"Batch tokenizing... (this will be much faster)\")\n",
    "#         # Tokenize everything at once\n",
    "#         self.encodings = tokenizer(\n",
    "#             df['text'].tolist(),\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=512,\n",
    "#             padding=False, #'max_length',\n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'input_ids': self.encodings['input_ids'][idx],\n",
    "#             'attention_mask': self.encodings['attention_mask'][idx],\n",
    "#             'label': self.labels[idx]\n",
    "#         }\n",
    "class LegalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        # Store labels as a simple list first\n",
    "        self.labels = df['label'].tolist()\n",
    "\n",
    "        print(\"Batch tokenizing... (this will be much faster)\")\n",
    "        # Tokenize everything at once\n",
    "        self.encodings = tokenizer(\n",
    "            df['text'].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding=False,     # Dynamic padding enabled\n",
    "            truncation=True,\n",
    "            # return_tensors='pt'  <-- REMOVED THIS (It causes the crash!)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensor HERE, for just this one item\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long) # Note: Trainer expects 'labels' (plural)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a358dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amlan/legal/joshi/bail/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/indic-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9505dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch tokenizing... (this will be much faster)\n",
      "Batch tokenizing... (this will be much faster)\n",
      "Batch tokenizing... (this will be much faster)\n",
      "Batch tokenizing... (this will be much faster)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LegalDataset(train_df, tokenizer)\n",
    "val_dataset = LegalDataset(val_df, tokenizer)\n",
    "hp_train_dataset = LegalDataset(hp_train_df, tokenizer)\n",
    "hp_val_dataset = LegalDataset(hp_val_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94cdcbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All datasets ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths on your Drive\n",
    "save_dir = \"/content/drive/MyDrive/bail_prediction_datasets/\" if IN_COLAB else 'pt_datasets/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "paths = {\n",
    "    \"train\": os.path.join(save_dir, \"train_dataset.pt\"),\n",
    "    \"val\": os.path.join(save_dir, \"val_dataset.pt\"),\n",
    "    \"hp_train\": os.path.join(save_dir, \"hp_train_dataset.pt\"),\n",
    "    \"hp_val\": os.path.join(save_dir, \"hp_val_dataset.pt\")\n",
    "}\n",
    "\n",
    "# Helper function to load or create\n",
    "def save_dataset(df, file_path):\n",
    "    torch.save(df, file_path)\n",
    "\n",
    "\n",
    "save_dataset(train_dataset, paths[\"train\"])\n",
    "save_dataset(val_dataset, paths[\"val\"])\n",
    "save_dataset(hp_train_dataset, paths[\"hp_train\"])\n",
    "save_dataset(hp_val_dataset, paths[\"hp_val\"])\n",
    "\n",
    "print(\"\\nAll datasets ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bail (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
