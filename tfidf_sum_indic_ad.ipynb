{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1489057c-0e87-47e8-97f7-0a630d6eadf9",
      "metadata": {
        "id": "1489057c-0e87-47e8-97f7-0a630d6eadf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/amlan/legal/joshi/bail/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d-7XBuJLEbb",
      "metadata": {
        "id": "2d-7XBuJLEbb"
      },
      "outputs": [],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1778e89-05f3-4fbd-a4ff-5b5f19e36ea8",
      "metadata": {
        "id": "a1778e89-05f3-4fbd-a4ff-5b5f19e36ea8"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizer\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b185d897-cde9-4e05-99ba-5a19efd91597",
      "metadata": {
        "id": "b185d897-cde9-4e05-99ba-5a19efd91597"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\"ai4bharat/indic-bert\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "14e5f6a6-251f-486c-9991-12a04862a442",
      "metadata": {
        "id": "14e5f6a6-251f-486c-9991-12a04862a442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b943277e-f9a4-424c-aaa3-74332f61b725",
      "metadata": {
        "id": "b943277e-f9a4-424c-aaa3-74332f61b725"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"train_all_ranked.csv\")\n",
        "test_df = pd.read_csv(\"dev_all_ranked.csv\")\n",
        "#train_df = train_df.head(500)\n",
        "#test_df = test_df.head(500)\n",
        "hp_train_df = train_df.sample(frac = 0.1, random_state=42).reset_index()\n",
        "hp_test_df = test_df.sample(frac = 0.1, random_state=42).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b147cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head(1)#['text']\n",
        "#test_df = test_df.head(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d1187d44-7724-4bbc-b33b-dcc002de74bd",
      "metadata": {
        "id": "d1187d44-7724-4bbc-b33b-dcc002de74bd"
      },
      "outputs": [],
      "source": [
        "class LegalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.df[\"text\"] = self.df[\"ranked-sentences\"].progress_apply(lambda x:\" \".join(eval(x)[:10]))\n",
        "        #self.df[\"label\"] = self.df[\"decision\"].progress_apply(lambda x:1 if x==\"granted\" else 0)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        model_input = self.df['text'][idx]\n",
        "        encoded_sent = self.tokenizer.encode_plus(\n",
        "            text=model_input,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            truncation=True\n",
        "            )\n",
        "\n",
        "        input_ids = encoded_sent.get('input_ids')\n",
        "        attention_mask = encoded_sent.get('attention_mask')\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "        label = torch.tensor(self.df['label'][idx])\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57773094-f2dd-4aae-9eb9-ffb3837f37de",
      "metadata": {
        "id": "57773094-f2dd-4aae-9eb9-ffb3837f37de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 123742/123742 [00:04<00:00, 30605.57it/s]\n",
            "100%|██████████| 17707/17707 [00:00<00:00, 32533.07it/s]\n",
            "100%|██████████| 12374/12374 [00:00<00:00, 37680.77it/s]\n",
            "100%|██████████| 1771/1771 [00:00<00:00, 34966.85it/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = LegalDataset(train_df, tokenizer)\n",
        "test_dataset = LegalDataset(test_df, tokenizer)\n",
        "hp_train_dataset = LegalDataset(hp_train_df, tokenizer)\n",
        "hp_test_dataset = LegalDataset(hp_test_df, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "167dbcbc-389f-45d4-80e0-b765e1dfac19",
      "metadata": {
        "id": "167dbcbc-389f-45d4-80e0-b765e1dfac19"
      },
      "outputs": [],
      "source": [
        "metric1 = evaluate.load(\"accuracy\")\n",
        "metric2 = evaluate.load(\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6c42716a-745e-4727-8e01-ea363abedea1",
      "metadata": {
        "id": "6c42716a-745e-4727-8e01-ea363abedea1"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric1.compute(predictions=predictions, references=labels)\n",
        "    f1 = metric2.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    return {'accuracy': accuracy[\"accuracy\"], 'f1-score': f1[\"f1\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "51569c0c-6377-403e-b2c9-ba33e8290bd9",
      "metadata": {
        "id": "51569c0c-6377-403e-b2c9-ba33e8290bd9"
      },
      "outputs": [],
      "source": [
        "def my_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
        "        \"weight_decay\":trial.suggest_float(\"weight_decay\", 0.005, 0.05),\n",
        "        \"adam_beta1\":trial.suggest_float(\"adam_beta1\", 0.75, 0.95),\n",
        "        \"adam_beta2\":trial.suggest_float(\"adam_beta2\", 0.99, 0.9999),\n",
        "        \"adam_epsilon\":trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "46cf81c2-18fe-4bde-b2c1-bb7b7a571c7a",
      "metadata": {
        "id": "46cf81c2-18fe-4bde-b2c1-bb7b7a571c7a"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='htf3_results',          # output directory\n",
        "    num_train_epochs=5,            # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,              # strength of weight decay\n",
        "    logging_dir='htf3_logs',           # directory for storing logs\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=250,\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit = 1,\n",
        "    learning_rate = 0.00001,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model =\"eval_f1-score\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "73102706-55c1-4c83-a6e1-34870dbfc1c9",
      "metadata": {
        "id": "73102706-55c1-4c83-a6e1-34870dbfc1c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_9588/2587673528.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model_init=model_init,                        # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=hp_train_dataset,         # training dataset\n",
        "    eval_dataset=hp_test_dataset,           # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "861aa998-6334-498a-b2e8-f12a58039340",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "861aa998-6334-498a-b2e8-f12a58039340",
        "outputId": "74963e56-3cfc-4032-9791-701aef035d18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-16 23:30:35,336] A new study created in memory with name: no-name-52bddbda-5b7e-4897-849e-97c591b6eb61\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7735' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7735/7735 1:09:39, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.638800</td>\n",
              "      <td>0.607652</td>\n",
              "      <td>0.659514</td>\n",
              "      <td>0.659514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.580900</td>\n",
              "      <td>0.620935</td>\n",
              "      <td>0.652739</td>\n",
              "      <td>0.652739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.522000</td>\n",
              "      <td>0.544120</td>\n",
              "      <td>0.727837</td>\n",
              "      <td>0.727837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.505300</td>\n",
              "      <td>0.525780</td>\n",
              "      <td>0.739695</td>\n",
              "      <td>0.739695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.486000</td>\n",
              "      <td>0.515317</td>\n",
              "      <td>0.762281</td>\n",
              "      <td>0.762281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 00:40:17,419] Trial 0 finished with value: 1.5245623941276114 and parameters: {'learning_rate': 1.073446762791212e-05, 'weight_decay': 0.013795256179277411, 'adam_beta1': 0.7738309599946543, 'adam_beta2': 0.9975279662623506, 'adam_epsilon': 8.982944128984163e-09}. Best is trial 0 with value: 1.5245623941276114.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7735' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7735/7735 1:09:37, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.639900</td>\n",
              "      <td>0.611689</td>\n",
              "      <td>0.629588</td>\n",
              "      <td>0.629588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.618600</td>\n",
              "      <td>0.605548</td>\n",
              "      <td>0.685488</td>\n",
              "      <td>0.685488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.587200</td>\n",
              "      <td>0.599063</td>\n",
              "      <td>0.696217</td>\n",
              "      <td>0.696217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.593800</td>\n",
              "      <td>0.587872</td>\n",
              "      <td>0.704122</td>\n",
              "      <td>0.704122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.563600</td>\n",
              "      <td>0.583000</td>\n",
              "      <td>0.702993</td>\n",
              "      <td>0.702993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 01:49:56,348] Trial 1 finished with value: 1.4059853190287972 and parameters: {'learning_rate': 1.4301355697080908e-06, 'weight_decay': 0.010381038116824762, 'adam_beta1': 0.8680002602350502, 'adam_beta2': 0.9972046863141097, 'adam_epsilon': 2.851091486451478e-09}. Best is trial 0 with value: 1.5245623941276114.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7735' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7735/7735 1:08:43, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.671400</td>\n",
              "      <td>0.660810</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.674000</td>\n",
              "      <td>0.659350</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.664500</td>\n",
              "      <td>0.660049</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.667100</td>\n",
              "      <td>0.660079</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.661500</td>\n",
              "      <td>0.660627</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 02:58:42,655] Trial 2 finished with value: 1.2557876905702994 and parameters: {'learning_rate': 4.8487858527449394e-05, 'weight_decay': 0.024920666818435015, 'adam_beta1': 0.8852178466691856, 'adam_beta2': 0.9983194186538753, 'adam_epsilon': 1.1118485615749815e-09}. Best is trial 0 with value: 1.5245623941276114.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7735' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7735/7735 1:09:22, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.617100</td>\n",
              "      <td>0.599269</td>\n",
              "      <td>0.661773</td>\n",
              "      <td>0.661773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.551800</td>\n",
              "      <td>0.546224</td>\n",
              "      <td>0.728402</td>\n",
              "      <td>0.728402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.503400</td>\n",
              "      <td>0.502397</td>\n",
              "      <td>0.770751</td>\n",
              "      <td>0.770751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.471100</td>\n",
              "      <td>0.511952</td>\n",
              "      <td>0.769622</td>\n",
              "      <td>0.769622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.446600</td>\n",
              "      <td>0.516840</td>\n",
              "      <td>0.776398</td>\n",
              "      <td>0.776398</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 04:08:07,410] Trial 3 finished with value: 1.5527950310559007 and parameters: {'learning_rate': 1.0375657886671487e-05, 'weight_decay': 0.04627390717704294, 'adam_beta1': 0.8107180784008142, 'adam_beta2': 0.9988716037246189, 'adam_epsilon': 1.253765545756664e-08}. Best is trial 3 with value: 1.5527950310559007.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7735' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7735/7735 1:08:35, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.675700</td>\n",
              "      <td>0.660067</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.675400</td>\n",
              "      <td>0.660345</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.666800</td>\n",
              "      <td>0.660274</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.665000</td>\n",
              "      <td>0.660082</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.661300</td>\n",
              "      <td>0.660066</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 05:16:44,361] Trial 4 finished with value: 1.2557876905702994 and parameters: {'learning_rate': 7.493435578036118e-05, 'weight_decay': 0.03032717254551403, 'adam_beta1': 0.8479593508676107, 'adam_beta2': 0.9991721937186858, 'adam_epsilon': 2.0711984126169957e-08}. Best is trial 3 with value: 1.5527950310559007.\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1547' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1547/7735 13:50 < 55:26, 1.86 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>0.659450</td>\n",
              "      <td>0.628458</td>\n",
              "      <td>0.628458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 05:30:36,253] Trial 5 pruned. \n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1547' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1547/7735 13:51 < 55:31, 1.86 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.637000</td>\n",
              "      <td>0.612885</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 05:44:29,544] Trial 6 pruned. \n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1547' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1547/7735 13:51 < 55:29, 1.86 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.670400</td>\n",
              "      <td>0.664296</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 05:58:22,184] Trial 7 pruned. \n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1547' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1547/7735 13:44 < 55:02, 1.87 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.671600</td>\n",
              "      <td>0.661912</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 06:12:08,243] Trial 8 pruned. \n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1547' max='7735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1547/7735 13:46 < 55:08, 1.87 it/s, Epoch 1/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.676200</td>\n",
              "      <td>0.659881</td>\n",
              "      <td>0.627894</td>\n",
              "      <td>0.627894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-17 06:25:55,846] Trial 9 pruned. \n"
          ]
        }
      ],
      "source": [
        "best_run = trainer.hyperparameter_search(n_trials=10,direction=\"maximize\",hp_space=my_hp_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "49be5278-a464-487d-b15d-04bbd6b0c927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49be5278-a464-487d-b15d-04bbd6b0c927",
        "outputId": "b86375a4-1da9-41a6-8d44-edb5a146c083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best HyperParameters\n"
          ]
        }
      ],
      "source": [
        "print(\"Best HyperParameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f33eaac8-878d-49d6-b75f-be7666736f67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "f33eaac8-878d-49d6-b75f-be7666736f67",
        "outputId": "3cfe037b-7e66-48c0-85af-9567da94720d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BestRun(run_id='3', objective=1.5527950310559007, hyperparameters={'learning_rate': 1.0375657886671487e-05, 'weight_decay': 0.04627390717704294, 'adam_beta1': 0.8107180784008142, 'adam_beta2': 0.9988716037246189, 'adam_epsilon': 1.253765545756664e-08}, run_summary=None)\n"
          ]
        }
      ],
      "source": [
        "print(best_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "52d2bacb-b8b3-4ade-ad31-3df04cfff6b2",
      "metadata": {
        "id": "52d2bacb-b8b3-4ade-ad31-3df04cfff6b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "del trainer\n",
        "del training_args\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7d1868ea-2697-49e5-95ec-87038d6e4897",
      "metadata": {
        "id": "7d1868ea-2697-49e5-95ec-87038d6e4897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting Training...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "016961c4-a64a-41a2-85b5-bc97d968ca2e",
      "metadata": {
        "id": "016961c4-a64a-41a2-85b5-bc97d968ca2e"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='tf3_results',          # output directory\n",
        "    num_train_epochs=15,            # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,               # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,              # strength of weight decay\n",
        "    logging_dir='tf3_logs',           # directory for storing logs\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=250,\n",
        "    save_strategy='epoch',\n",
        "    save_total_limit = 1,\n",
        "    learning_rate = 0.00001,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model =\"eval_f1-score\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bb13ccc5-14c5-4d51-ac66-1428260e6d02",
      "metadata": {
        "id": "bb13ccc5-14c5-4d51-ac66-1428260e6d02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_9588/897991326.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model_init=model_init,                        # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,           # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2d9c54ab-3406-4d09-9742-ff1a95e45423",
      "metadata": {
        "id": "2d9c54ab-3406-4d09-9742-ff1a95e45423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.8107180784008142,\n",
            "adam_beta2=0.9988716037246189,\n",
            "adam_epsilon=1.253765545756664e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=epoch,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=no,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1.0375657886671487e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=tf3_logs,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=250,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=eval_f1-score,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=15,\n",
            "optim=adamw_torch_fused,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=tf3_results,\n",
            "overwrite_output_dir=False,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "project=huggingface,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=[],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=None,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "trackio_space_id=trackio,\n",
            "use_cpu=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.04627390717704294,\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='121' max='232020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   121/232020 01:00 < 32:40:10, 1.97 it/s, Epoch 0.01/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(trainer\u001b[38;5;241m.\u001b[39margs, n, v)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/legal/joshi/bail/.venv/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/legal/joshi/bail/.venv/lib/python3.10/site-packages/transformers/trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for n, v in best_run.hyperparameters.items():\n",
        "    setattr(trainer.args, n, v)\n",
        "print(trainer.args)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69b3b80-7013-4e1d-ae8f-54b10a68918a",
      "metadata": {
        "id": "c69b3b80-7013-4e1d-ae8f-54b10a68918a"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/home2/username/legal-tech/tfidf_sum+indic-ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bc1ada-9468-474c-b9d3-190ea806fabf",
      "metadata": {
        "id": "b5bc1ada-9468-474c-b9d3-190ea806fabf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bail (3.10.18)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
